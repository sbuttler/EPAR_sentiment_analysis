{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a62eee-e7dc-426f-9054-08f4d81975b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8d6146-6805-418c-87b2-a25d8f79c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2155ea69-d03a-4e24-a772-9f92e997d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(line):\n",
    "    \"\"\" Cleans a given line of text by converting to lowercase, removing numbers, whitespaces and punctuation\n",
    "        Parameters\n",
    "        ----------\n",
    "        line :  string\n",
    "                A singe line of text\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        clean_line :  string\n",
    "                      A cleaned line of text\n",
    "    \"\"\"\n",
    "    \n",
    "    # make all text lower case\n",
    "    clean_line = line.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    clean_line = \"\".join([ch for ch in clean_line if ch not in punct])\n",
    "\n",
    "    # remove numbers left over from enumeration\n",
    "    clean_line = re.sub(r\"[0-9\\n]\", \"\", clean_line)\n",
    "\n",
    "    #clean whitespace at beginning and end of each line\n",
    "    clean_line = clean_line.strip()\n",
    "\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ab1293-286d-4397-8026-28a7b05376a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_benefit_section_name(filepath):\n",
    "    \"\"\" Get the section names that contain keywords 'risk' or 'benefit' from EPAR table of contents\n",
    "        Parameters\n",
    "        ----------\n",
    "        filepath :  string\n",
    "                    The path to the EPAR pdf file.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        section_name :  list of strings\n",
    "                        The EPAR section names containing the keywords 'risk' or 'benefit'\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    output_string = StringIO()\n",
    "\n",
    "    # read text from pdf\n",
    "\n",
    "    with open(filepath, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        # assumption: table of contents is located at 2nd page of document\n",
    "        page = next(itertools.islice(PDFPage.create_pages(doc),1,2))\n",
    "        interpreter.process_page(page)\n",
    "    \n",
    "    # create dataframe for table of contents only containing lines that have text\n",
    "    toc = pd.DataFrame(\"\".join(s for s in output_string.getvalue().splitlines(True) if re.findall(\"[a-zA-Z]+\", s)).splitlines(), columns=['text_raw'])\n",
    "    \n",
    "    # clean text\n",
    "    toc['text_clean'] = toc.text_raw.apply(clean_text)\n",
    "    \n",
    "    # search for sections containing keywords 'risk' and 'benefit'\n",
    "    toc['relevant_section'] = (toc['text_clean'].str.contains('risk'))|(toc['text_clean'].str.contains('benefit'))\n",
    "    \n",
    "    return toc.loc[toc.relevant_section,'text_clean']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5573e199-192a-491e-b8c9-6f81b99e1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sentences_from_section(section_name, filepath):\n",
    "    \"\"\" Get sentences from specified section\n",
    "        Parameters\n",
    "        ----------\n",
    "        section_name :  string\n",
    "                        The name of the section\n",
    "        filepath :      string\n",
    "                        The path to the EPAR pdf file.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        section_sentences :  list of strings\n",
    "                             The sentences contained in the section\n",
    "    \"\"\"\n",
    "    \n",
    "    output_string = StringIO()\n",
    "\n",
    "    # read text from pdf\n",
    "\n",
    "    with open(filepath, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for i, page in enumerate(PDFPage.create_pages(doc)):\n",
    "            #skip title and table of contents page\n",
    "            if i < 2:\n",
    "                continue\n",
    "            interpreter.process_page(page)\n",
    "    \n",
    "    # create dataframe for document text\n",
    "    df_text = pd.DataFrame(\"\".join(s for s in output_string.getvalue().splitlines(True) if re.findall(\"[a-zA-Z]+\", s)).splitlines(), columns=['text_raw'])\n",
    "\n",
    "    # clean text\n",
    "    df_text['text_clean'] = df_text.text_raw.apply(clean_text)\n",
    "    \n",
    "    # search for relevant sections\n",
    "    df_text['is_relevant_section_title'] = (df_text['text_clean'].str.contains(section_name))\n",
    "    \n",
    "    # search for start of relevant section - is equal to index of first apppearance of relevant section plus 1\n",
    "    sec_start = df_text.loc[df_text.is_relevant_section_title,'text_clean'].index[0]+1\n",
    "    \n",
    "    relevant_text = \"\".join(text for text in df_text.loc[sec_start:,'text_raw'])\n",
    "    \n",
    "    relevant_sentences = pd.DataFrame(nltk.sent_tokenize(relevant_text), columns=['sentence'])\n",
    "    \n",
    "    return relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6938daf-48c7-47a3-b5ed-29ceee5327c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['WC500057122.pdf', 'WC500135744.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79486916-b8f6-40f1-97d5-1f203438c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and save sentences for both files\n",
    "\n",
    "for file in filenames:\n",
    "    \n",
    "    filepath = f'data/{file}'\n",
    "    section_name = get_risk_benefit_section_name(filepath).iloc[0]\n",
    "    sentences = get_relevant_sentences_from_section(section_name, filepath)\n",
    "    sentences.to_csv('data/sentences_{}.csv'.format(file.strip('.pdf')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f6175-c661-4f26-b486-e9d69d7f4aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayer",
   "language": "python",
   "name": "bayer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
